\documentclass{article}		%Petite taille de doc
\usepackage{fullpage}
\usepackage[francais]{babel}	%Doc fr
\usepackage{float}
\usepackage{todonotes}
\usepackage{tikz}
\usepackage{framed}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{euler}
\usepackage{fontspec}
\setmainfont[Ligatures={Historic},Numbers=OldStyle]{Linux Libertine O}

\newtheorem{defi}{Définition}
\theoremstyle{definition}
\newtheorem{theo}{Théorème}
\theoremstyle{plain}
\newtheorem{intui}{Ébauche de preuve}
\theoremstyle{plain}
\newtheorem{coro}{Corollaire}
\theoremstyle{plain}
\newtheorem{lemma}{Lemme}
\theoremstyle{plain}
\newtheorem{remark}{Remarque}
\theoremstyle{plain}

\begin{document}
\title{Étude du problème \emph{Learning Parities with
Noise}}
\author{Thomas Bourgeat}
\date\today
\maketitle
\section{Introduction au problème du LPN}
De plus en plus d'applications cryptographiques ont besoin d'être embarquées dans des dispositifs très économiques, voir jetables. Le LPN est un candidat
pour permettre de faire de la cryptographie, classiquement de
l'authentification, avec une puissance de calcul
très limitée et une diversité d'opérations très réduite.
Le standard AES est classiquement utilisé pour sa relative sécurité,
cependant les implantations sont assez coûteuses, et si le LPN s'avérait suffisamment
robuste, il serait une bonne alternative.
\\\\
Une fois un "bon" problème candidat à une application cryptographique, il
faut construire des protocoles pour être capable de lui faire faire de la
signature, du chiffrement ou de l'authentification. Ces protocoles ne
doivent pas introduire de faiblesses dans le système utilisé : ils doivent
être aussi difficile à casser que le problème initialement choisit est
difficile à résoudre.
Dans la suite nous étudierons d'abord des schémas d'authentification et du
chiffrement. Puis nous donnerons une idée de la difficulté du
problème du LPN par une vision des algorithmes actuels et plusieurs
réductions à d'autres problèmes connus. Cette méthode de travail par
étude des protocoles designés pour le LPN puis des algorithmes qui
résolvent le LPN va nous permettre de mieux cerner le problème. Ainsi on
isolera ce qui font \emph{les} difficulté du LPN et les précautions d'emploi. 
\paragraph{Notation et mise en forme}
Dans ce document certains résultats sont isolés. Ce choix a été fait pour
mettre en évidence certains résultats assez remarquables. Notamment
certain de ces résultats
se trouvent parfois dans un contexte plus général que le LPN. 
\subsection{Définition}
Dans la suite on travaillera, sauf mention explicite du contraire, dans
$GF(2)^n$, avec n la taille de clef. On note $Ber(\epsilon)$
une loi de Bernoulli de paramètre $\epsilon$ : qui vaut 1 avec
probabilité $1-\epsilon$. On rappelle que $\epsilon$ est petit : c'est-à-dire $\epsilon < \frac {1}{2}$.  
\begin{defi}
Soit $m \in \mathbb{N}, A \in \mathcal{M}_{m, n}(GF(2)), s \in GF(2)^n$ et
$\nu \in GF(2)^m$ tel que $\nu_i \leftarrow Ber(\epsilon)$. On appelle
problème du LPN le fait de retrouver $s$ à partir de $A, \epsilon,
A.s+\nu$. 
\end{defi}
Géométriquement, on dispose de $m$ projections légèrement bruitées d'un vecteur secret sur m
vecteurs donnés, et on veut retrouver le vecteur secret.
\subsection{Spécificité du problème}
On commence par remarquer que si le bruit est nul, il est très facile de
résoudre le problème par un algorithme d'élimination de Gauss-Jordan.
Donc certaines instances sont très faciles. De même une structure
excessive sur la matrice A rend le problème simple : si chaque vecteur de
la base canonique est mis en ligne dans A, chacun un nombre de fois
suffisant, on peut alors résoudre le problème par un simple vote à la 
majorité.
\\
Pour faire un parallèle on peut penser au problème de factorisation d'un
nombre comme produit de deux premiers : la difficulté d'une instance à
l'autre varie beaucoup, ainsi il n'est pas immédiat de choisir une instance
que l'on sait difficile à résoudre. 
\\
Bien que la difficulté du LPN ne soit pas connu, on verra un ensemble de
problèmes auquel il est relié. Ces problèmes ont des complexités mieux
connues et permettent de justifier l'avis assez partagé de la communauté
que c'est un problème difficile.

\section{Protocoles et preuves de sécurités}
\subsection{Chiffrement}
\subsubsection{Asymétrique}
\cite{Alekh}  propose deux cryptosystèmes à clef publique. Ils sont tous
les deux basés sur des conjectures d'indistinguabilité.
\paragraph{Cryptosystème 1}
Soient $k=n^{1/2-\epsilon}, m=2.n$. Ce premier cryptosystème ne chiffre
qu'un seul bit par étape. La procédure de génération de clef est comme
suit :
\begin{enumerate}
\item Générer une matrice $m\times n$ de $GF(2)$ aléatoire.
(les coefficients sont des $Ber_{1/2}$)
\item Générer un vecteur aléatoire de bruit $\nu$, de poids de hamming k,
et construire $b=A.x+e$ où $x$ suit une loi uniforme.
\item La clef est publique est alors $(A,b)$ et la clef privée
$(A,b,\nu)$
\end{enumerate}
\subparagraph{Chiffrement}

\begin{itemize}
\item Si on veut chiffrer $1$, envoyer un vecteur $\xi_1$ tiré selon une loi
uniforme
\item Si on veut chiffrer $0$, envoyer $\xi_0=y+e'$ où $e'$ est un mot aléatoire
de poids k, et $y\in Ker((b|A)^t)$
\end{itemize}

\subparagraph{Déchiffrement}
On calcul $\delta= e^t.\xi$, si $\delta=0$ on déchiffre $0$, sinon on
déchiffre $1$.


\begin{remark}
Ce cryptosystème se repose sur le LPN d'une manière un peu différente de
tout ce qu'on verra par la suite. Si on suppose que l'on connait le
nombre d'erreurs par authentification (groupe de m samples), il existe une attaque simple du
LPN : on regarde la parité de ce nombre, on xor toutes les m équations
entre elles, si il y a un nombre pair d'erreur, on ne change pas
l'équation finale, sinon on la flip. Dans les deux cas on est ainsi sur
d'avoir généré une équation juste. En répétant l'opération environ n
fois, on aurait résolu le LPN. Autrement dit, la connaissance de la
parité du nombre d'erreurs dans le LPN est une information très
importante. Alors que dans ce cryptosystème, elle est donnée.
\end{remark}

\begin{theo}
Ce cryptosystème est correct. Il est sécurisé sous l'hypothèse (conjecture 3.
de \cite{Alekh}) que la loi uniforme sur
les vecteurs à distance $k$ de l'image d'une matrice aléatoire et la loi
uniforme sur ceux à distance $k+1$ sont
indistinguables. \cite{Alekh} montre que sous les hypothèses de cette
conjecture, la loi uniforme sur les vecteurs à distance $k$ de l'image
d'une matrice aléatoire est indistinguable d'une loi uniforme.


\end{theo}

\begin{framed}
Un résultat nécessaire pour la démonstration :
\begin{defi}
Soit $\xi_2, \xi_2$ deux variables
aléatoires, 
 $\rho(\xi_1,\xi_2)= max_A(x)
|Pr(A(\xi_1)=1)-Pr(A(\xi_2)=1)|$ s'appelle la distance statistique sous
le test A. 
\\\\
On dit que deux suites $(\xi_{1,n})_n, (\xi_{2,n})_n$ de distributions sont computationnellement $f(n)$-proches,
si pour tout algorithme polynomial C, il existe $N$ tel que 
$$\forall n>N, |Pr(C(\xi_{1,n})=1)-Pr(C(\xi_{2,n})=1)|\leq f(n)$$
On note alors $\rho_c(\xi_1,\xi_2) \leq f(n)$.
\\
Enfin on dit que des distributions sont computationellements
indistinguables si leur distance est inférieur à $\frac{1}{n^{\Omega(1)}}$


\end{defi}
\begin{theo}
L'indistinguabilité est transitive : soit $\xi_1,\xi_2,\xi_3$ trois suites de
variables aléatoires telles que 
$$\begin{cases}
\rho_c(\xi_1,\xi_2)\leq f_1(n)\\
\rho_c(\xi_2,\xi_3)\leq f_2(n)
\end{cases}
$$
Alors on a :
$$\rho_c(\xi_1,\xi_3)\leq f_1(n)+f_2(n)$$
\end{theo}

\begin{proof}
Conséquence de : 
$$|(a-b)-(c-b)|\leq|a-b|+|c-b|$$
\end{proof}
\end{framed}

\begin{proof}
\begin{itemize}
\item \emph{Correction}
\\
La probabilité que $e^t.\xi_1=0$ est exactement $\frac{1}{2}$. Or la
probabilité que $e^t.\xi_0=0$ est différente :
\begin{center}
\begin{tabular}{c c c}
   $e^t.\xi_0$ &$ =$ &$ e^t.(y+e') $\\
      & $=$&$ e^t.e'$
  \end{tabular}
 \end{center}
Or $e$ et $e'$ sont aléatoires de poids $k$, la probabilité qu'ils ont un 1
en commun est inférieur à $(1-\frac{k}{n})^k=o(1)$.
Autrement dit pour pouvoir utiliser ce cryptosystème il faut réitérer le
processus, pour que le destinataire puisse différentier des Bernoulli 1/2
de Bernoulli $o(1)$.
\item \emph{Intuition de sécurité}
\\
Tout d'abord, $(b|A)$ est indistinguable d'une matrice aléatoire
$\hat{A}$. 


Soit $\hat{\xi}$ un vecteur tiré aléatoire tiré selon la même
loi que $\xi_0$ sauf que $\hat{y}\in Ker(\hat{A})$. Alors $(\xi_0,(b|A))$
et $(\hat{\xi},\hat{A})$ sont indistinguables.
De plus, on peut voir le noyau de $\hat{A}$ comme l'image d'une matrice
$B$, donc $\hat{\xi}=B.z+\hat{e'}$ où $z$ est tiré uniformément dans tout
l'espace de départ de B, et 
$\hat{e'}$ uniformément dans les mots de poids k et de taille m.
\\
Ainsi il est difficile de distinguer $(\hat{\xi},\hat{A})$ de
$(\xi_1,\hat{A})$, puisqu'on est dans le cadre de la conjecture.
Enfin il est difficile de distinguer $(\xi_1,\hat{A})$ de $(\xi_1,
(b|A))$ d'après le début de la preuve.
\\\\
Donc au final, il est donc difficile de
distinguer $(\xi_0,(b|A))$ de $(\xi_1 ,(b|A) )$.
\end{itemize}
\end{proof}


\paragraph{Cryptosystème 2 : utilisation d'un bon code correcteur}

Soit $m=2.n, k=n^{\frac{1}{2}-\epsilon}$, et H la matrice de parité
(noyau) d'un bon code correcteur d'erreur pour lequel il existe un
algorithme de décodage efficace\footnote{Par exemple un LDPC.}.
\\\\Génération des clefs :
\begin{itemize}
\item $A\in_U \mathcal{M}_{m,n}(GF(2))$, $X\in_U
\mathcal{M}_{n,m}(GF(2))$, $E\in \mathcal{M}_{m,m}(GF(2))$ où $E$
contient exactement $k$ 1 par lignes et ces matrices sont telles que
$M=A.X+E$ soit inversible. (On recommence sinon)
\item On appelle $V=Ker(A^t.(M^t)^{-1})\cap Ker(H)$ $r=dim(v)$.\item On
suppose sans perte de généralité que r est pair et on choisit une
partition $V=V_1 \oplus V_2$ telle que $dim(V_1)=dim(V_2)=r/2$
\item La clef publique est $(M,A,V_1,V_2)$. 
\item La clef privée est $(M,A,V_1,V_2,E)$.
\end{itemize}

\subparagraph{Chiffrement}
Un message à chiffrer fait $r/2$ bits et on l'identifie avec un vecteur de
$v_1 \in V_1$. Pour le chiffrer calculer $\xi (v_1)=
(M^t)^{-1}(v_1+v_2)+e'$ où $e'$ est un vecteur aléatoire de poids k et où
$v_2$ et choisit uniformément dans $V_2$.

\subparagraph{Déchiffrement}
\begin{itemize}
\item
Calculer $\hat{v}=E^t.\xi$
\item Trouver le mot code $v$ le plus proche de
$\hat{v}$ en utilisant l'algorithme de décodage. 
\item Projeter v sur $V_1$ pour retrouver le mot message initial.
\end{itemize}

\begin{intui}
Ce schéma est correct avec grande probabilité. En effet 
\begin{center}
\begin{tabular}{r c l}
 $E^t.\xi - (v_0+v_1)$ & $=$&
$E^t.((M^t)^{-1}.(v_0+v_1)+e')-M^t(M^t)^{-1}(v_0+v_1)$ \\
& $=$ & $(E^t-M^t).(M^t)^{-1}(v_0+v_1)+E^t.e'$\\
& $=$ & $X^t.A^t.(M^t)^{-1}(v_0+v_1)+E^t.e'$\\
& $=$ & $E^t.e'$
\end{tabular}
\end{center}
Et alors vu qu'il n'y a que k bits à 1 par lignes de E, on obtient un mot
de poids de hamming faible avec forte probabilité, qui permet de trouver
le bon mot code grâce au code correcteur d'erreur. 
\end{intui}
\paragraph{Faisabilité (à titre informatif)}
\cite{Dam} propose une comparaison entre une implémentation de RSA 1024
bits et un LPN $n=768,\eta=0.015$. Les performances du LPN sont moins
bonnes, entre un facteur 10 et un facteur 100. Cependant \cite{Dam} dit
que ce n'est pas redhibitoire. Néanmoins, les paramètres choisis dans
\cite{Dam} sont criticables. En effet ils ont choisi de se baser sur
\cite{LF} qui propose un algorithme performant pour des bruits
importants. Le choix de $\eta=0.015$ donne l'avantage à des algorithmes
par recherche exhaustive type \cite{Valiant},\cite{Grigo} ou même plus
concret \cite{Gol}. Ainsi la sécurité annonçée n'est pas du tout de
$2^{80}$.

\subsubsection{Symétrique : LPN-C}

Dans sa thèse Yannick Seurin a proposé un schéma de chiffrement
symétrique basé sur le LPN. On va voir qu'ici le schéma semble utiliser
le LPN différement de \cite{Alekh}.


\subsection{Authentification clef privé}
Le LPN est souvent proposé pour faire de l'authentification et la majorité
de la littérature traite de l'authentification. Dans la suite on appelle
Tag le dispositif qui souhaite s'authentifier, et Reader celui auprès de qui le
Tag essaie de s'authentifier.

Cette partie est essentiellement un rapport des articles \cite{Lyu} et
\cite{Katz} dont la lecture consistait une partie de mon mémoire d'info-math.

\subsubsection{HB}
L'exemple le plus simple consiste à choisir le squelette suivant, très
classique, d'authentification :
\begin{enumerate}
\item On détermine un challenge que le Tag doit faire, qui est soluble
seulement si le Tag possède la clef. L'instance peut-être choisit soit
pas le Reader, soit au hasard, par le Tag.
\item Le Tag envoit sa réponse au Reader (avec éventuellement le
challenge si c'est une instance qu'il a tiré)
\item Le Reader, possédant aussi la clef secrète vérifie que le Tag a
bien répondu au challenge. Si c'est le cas il authentifie le Tag.
\end{enumerate}


Ce schéma s'instancie de manière quasi-transparente grâce au LPN. Il
faut simplement rajouter une couche probabiliste, et on obtient le
protocole HB : figure~\ref{hb}. L'idée est donc de répéter l'opération,
et de voir si le Tag réussi une proportion environ $1+\epsilon$ des
challenges. Si c'est le cas, on accepte l'authentification.
\\Le schéma est paramétré par $l,u$ les lower et upper bound
d'acceptation du schéma. Attention momentanément $k$ est la taille de
clef, et $n$ est le nombre de répetitions du schéma.


\begin{figure}
\centering
\begin{tikzpicture}
\draw (-0.5,1) node[above] {Tag$(s,\epsilon)$};
\draw (2.5,1) node[above] {Reader$(s,\epsilon)$};
 \draw[<-] (0,0) -- (2,0);
\draw[->] (0,-1) -- (2,-1);
\draw (1,0) node[above] {$a$};
\draw (1,-1) node[above] {$z$};
\draw (3,0) node {$a\leftarrow\{0,1\}^n$};
\draw (-1,-0.75) node {$\nu\leftarrow Ber(\epsilon)$};
\draw (-1.,-1.25) node {$z:= a.s+\nu$};
\draw (3,-1) node {$z\stackrel{?}{=}a.s$};
\end{tikzpicture}
\caption{Protocole classique HB}
\label{hb}
\end{figure}
\paragraph{Sécurisé dans le modèle passif}
Le but des preuves de sécurité est de montrer qu'un protocole est aussi
difficile à casser que le problème du LPN est difficile à résoudre.

Le modèle passif consiste à suppose que l'attaquant ne peut pas intéragir
avec les protagonistes. Ainsi une attaque dans le modèle passif consiste
à dire que l'attaquant regarde des authentification, et au
bout d'un certains temps il dit "Je suis capable de faire une fausse
authentification avec probabilité $>1/2$".

 On va montrer que pour le protocole HB, si tel est
le cas, alors l'attaquant peut aussi résoudre le LPN. Ceci donnera donc
une garantie de sécurité du protocole : aussi difficile que le LPN dans
le modèle passif.

\begin{framed}
Pour la plupart des preuves de sécurité qui vont suivre, on va utiliser
le lemme suivant, qui ramène le problème du LPN a un problème de
distinguabilité.

\begin{theo}Lemme technique \label{lemmetechnique}\\
Supposons qu'il existe un algorithme D qui en faisant $q$ queries,
s'exécutant en un temps t, arrive à distinguer une distribution uniformé
d'une distribution sortie d'un oracle LPN :
$$|Pr(s\leftarrow\{0,1\}^n : D^{A_{s,\epsilon}}(1^k)=1)
-Pr(D^{U}(1^k)=1)|\geq \delta $$

Alors il existe un algorithme M qui résout le LPN en temps
$O(t.k.\delta^{-2}.log(k))$:
$$Pr(M^{A_{s,\epsilon}}(1^k)=s)\geq\delta/4 $$
\end{theo}
\begin{proof}
\todo[inline]{PReciser N}
L'algorithme M est le suivant :
\begin{enumerate}
\item M choisit les évènements d'aléa $\omega$ pour D de manière
représentative, qui sera fixé pour toute
l'exécution. (car D peut être randomisé).
\item M execute $D$ sur l'oracle uniforme, 
il l'exécute N fois pour obtenir une estimation de la probabilité que D
renvoit 1.
\item M demande q.N samples de l'oracle LPN : $\{(a_{1,j},z_{1,j})\}_{j=1}^q,
\dots,\{(a_{N,j},z_{N,j})\}_{j=1}^q$ Alors pour tous les bits de clef il
fait 
\begin{enumerate}
\item Exécution N fois de D, en
utilisant un ensemble frais de samples $\{(a_{j},z_{j})\}_{j=1}^q$ pour
répondre aux
queries que D fait à l'oracle à chaque fois.  Pour répondre à la $j$-ième
querie de D de chaque itération, choisir un bit aléatoire $c_j$ et
renvoyer $(a_j \oplus (c_j.e_j),z_j)$ où les $e_j$ sont les vecteurs de
la base canonique. Obtenir une estimation $p_i$ de la probabilité que D
renvoit $1$ avec un tel oracle.
\item Si $|p_i-p| \geq \delta/4$ alors $s'_i=0$ sinon $s'_i=1$
\end{enumerate}
\item Renvoyer $s'=(s'_1,\dots,s'_k)$
\end{enumerate}

\emph{Correction de l'algorithme:}
\\
Tout d'abord par un argument d'espérance, avec probabilité au moins
$\delta/2$ sur le choix de s et d'évènements $\omega$, on a :
$$|Pr[D^{A_{s,\epsilon}}(1^k,\omega)=1]-Pr[D^{U_{k+1}}(1^k,\omega)|\geq
\frac{\delta}{2}$$
L'idée est de se restreindre au $s,\omega$ pour lesquels cette équation est
valide et montrer que dans ces cas là, avec probabilité au moins 1/2,
l'algorithme fonctionne.
\\
D'une part, on a avec probabilité $1-O(1/k)$ :$$|Pr[D^{U_{k+1}}(2^k,\omega)=1]-p|\leq \delta/16 $$
Puis si on appelle $hyb_i$ l'oracle fabriquée dans l'étape 3 pour le bit
i de clef, on a avec
probabilité $1-O(1/k)$:
$$|Pr[D^{hyb_i}(1^k,\omega)=1]-p_i|\leq \delta/16 $$
Ainsi via Union Bound (on passe au complémentaire), on a avec probabilité au moins $1/2$, les deux
dernières équations simultanément vraies. On suppose qu'on est dans ce
cas.\\
\\
On remarque que si $s_i=0$ alors $hyb_i=A_{s,\epsilon}$ tandis que si
$s_i=1$ alors $hyb_i=U_{k+1}$. Et donc :
\begin{itemize}
\item Si $s_i=0$ par la toute première
équation de la preuve on a:
$$ |Pr[D^{hyb_i}(1^k,\omega)]-Pr[D^{U_{k+}}(1^k,\omega)=1]|\geq \delta/2$$ 
et donc $|p_i-p|\geq
\frac{\delta}{2}-2.\frac{\delta}{16}=3\frac{\delta}{8}>\frac{\delta}{4}$ donc
$s'_i=s'i=0$.
\item Si $s_i=1$ 
$$ |Pr[D^{hyb_i}(1^k,\omega)]=Pr[D^{U_{k+}}(1^k,\omega)=1]|$$ donc
$|p_i-p|\leq 2.\frac{\delta}{16} =\frac{\delta}{8} <\frac{\delta} {4}$ et
donc $s'_i=s_i=1$.
\end{itemize}
On a donc prouvé qu'avec probabilité au moins
$\frac{\delta}{2}.\frac{1}{2}$ l'algorithme renvoyait le bon secret.
\end{proof}
\end{framed}

\begin{framed}
Autre notion que l'on va beaucoup utiliser, la notion d'avantage pour un
adversaire.
\begin{defi}
On définit l'avantage (dans le modèle passif pour le moment) d'un adversaire sur le
protocole HB comme : 
$$Adv_{\mathcal{A}} \stackrel{def}{=} Pr[\text{Reader accepte une
tentative de l'attaquant}]-\delta^{\star}_{\epsilon,s,l,u,n}$$
où
$\delta^{\star}_{\epsilon,s,l,u,n}=2^{-n}.\sum\limits_{i=l \dots
u}\binom{n}{i}$ qui représente la propabilité de réussite en essayant au
hasard. Dit d'une manière différente, cette quantité évalue
l'\emph{avantage} que peut avoir un adversaire par rapport à faire une
authentification au hasard.
\end{defi}
\end{framed}
\begin{theo}
Supposons qu'il existe un adversaire $\mathcal{A}$ qui avec au plus $q$
exécutions du protocole HB, s'exécute en un temps t et acquiert un
avantage $Adv_{\mathcal{A}}(\epsilon,l,u,n)\geq \delta$. alors il existe
un algorithme D, utilisant $(q+1).n$ queries, s'exécutant en $O(t)$ et
tel que 
$$|Pr[D^{A_{s,\epsilon}}(1^k)=1]-Pr[D^{U_{k+1}}(1^k)=2]|\geq \delta +
\delta^{\star}_{\epsilon,l,u,n}-\epsilon_c-2^{-n}.\sum_{i=0}^{2.u}
\binom{n}{i}$$
avec $\epsilon_c$ la probabilité d'un Tag honnête soit
rejeté\footnote{Donc
il ne faut pas prendre $l$ et $u$ trop resserrés autour de $\epsilon.n$ pour éviter
de rejeter des bon tags.}.
\end{theo}
\begin{proof}
Grossièrement, l'idée de ce type de preuve et de montrer que s'il existe un tel
attaquant, alors cet attaquant peut fabriquer un algorithme pour
distinguer un oracle LPN d'un oracle random. Donc pour ça il va simuler un protocole HB, et l'attaquer. La
simulation sera choisie dépendant de l'instance LPN à résoudre  de telle manière que réussir à authentifier sera
la même chose que distinguer un LPN d'un uniforme.
\\\\ Étant donné l'accès à un oracle qui retourne les échantillons
$(a,z)$. On procède de la façon suivante: 
\begin{enumerate}
\item D exécute la première phase de $\mathcal{A}$. Chaque fois que
l'attaquant demande à voir un protocole, D obtient les samples et les
donne à l'attaquant.
\item Quand l'attaquant est prêt, D récupère n autres samples de l'oracle
: $\{(\bar{a_i},\bar{z_i})\}_{i=1}^{n}$. D envoit le challenge ainsi récupéré à
l'attaquant, qui donne une réponse $Z'=(z'_1,\dots,z'_n)$
\item D répond $1$ si $\bar{Z}$ diffère de $Z'$ pour au plus 2u entrées.
\end{enumerate}
Moralement, l'algorithme consiste à tester l'attaquant : si il échoue de
beaucoup, c'est qu'on lui a pas donné une bonne entrée, donc que c'est un
uniforme. Si il réussi ou presque, c'est qu'on lui a donné un vrai oracle LPN.
\\
De manière plus calculatoire : si l'oracle est aléatoire, l'analyse est
rapide. Si c'est un vrai oracle LPN : avec probabilité au moins
$\delta+\delta^{\star}_{\epsilon,l,u,n}$ $Z'$ et
$Z^{\star}=(s.\bar{a_1},\dots,s.\bar{a_n})$ diffèrent
d'au plus u entrées. De plus $\bar{Z}$ et $Z^{\star}$ diffèrent d'au plus
u positions avec probabilité $1-\epsilon_c$. Ce qui nous permet de
conclure.
\end{proof}
\paragraph{Une attaque active contre HB}
Dans le modèle actif il y a une première phase en plus, durant laquelle
l'attaquant peut intéragir avec le Tag.
\\\\
 Si on rajoute cette étape, 
il y a une attaque active simple. On peut commencer par donner plusieurs
fois le même $a$, vecteur de la base canonique, au Tag, qui va répondre,
et ainsi nous donner un bit de clef. Puis on recommence pour les autres
bits de clef.
\\\\ 
C'est un exemple de cas où la couche de protocole est la faiblesse du
système : elle a rendu inutile la difficulté du LPN. 
\subsubsection{HB\#}
Pour corriger le défaut du protocole HB, l'idée est de faire un protocole
a deux clefs, qui empechera la première phase d'une attaque active
d'apporter trop d'information. Le protocole est présenté dans la
figure~\ref{hbsharp}. Le deux clefs font une taille $k$. Et momentanément
$n$ est le nombre de répétitions du protocole. 
\begin{figure}
\centering
\begin{tikzpicture}
\draw (-0.5,1) node[above] {Tag$(s_1,s_2,\epsilon)$};
\draw (2.5,1) node[above] {Reader$(s_1,s_2,\epsilon)$};
 \draw[->] (0,0.25) -- (2,0.25);
 \draw[<-] (0,-0.25) -- (2,-0.25);
\draw[->] (0,-1) -- (2,-1);
\draw (1,0.25) node[above] {$b$};
\draw (1,-0.25) node[above] {$a$};
\draw (1,-1) node[above] {$z$};
\draw (-1,0.25) node {$b\leftarrow\{0,1\}^k$};
\draw (3,0) node {$a\leftarrow\{0,1\}^k$};
\draw (-1,-0.75) node {$\nu\leftarrow Ber(\epsilon)$};
\draw (-1.,-1.25) node {$z:= a.s_1+b.s_2+\nu$};
\draw (3.5,-1) node {$z\stackrel{?}{=}a.s_1+b.s_2$};
\end{tikzpicture}
\caption{Protocole HB\#}
\label{hbsharp}
\end{figure}
\paragraph{Sécurisé dans le modèle actif}
Le protocole ainsi modifié est sécurisé dans un modèle actif, comme le
dit le théorème qui va suivre.
\cite{Katz} est à l'origine de cette preuve assez simple de sécurité.
\begin{defi}
On appelle avantage d'un adversaire $\mathcal{A}$ dans le modèle actif la
quantité :
$$Adv_{\mathcal{A}}^{\text{Actif}} \stackrel{def}{=} Pr[\text{Reader accepte une
tentative de l'attaquant après la phase active}]-\delta^{\star}_{\epsilon,s,l,u,n}$$
\end{defi}

\begin{theo}
S'il existe un adversaire $\mathcal{A}$ qui interagit avec le Tag au plus $q$ fois
 et qui a un avantage
$Adv_{\mathcal{A}}^{\text{Actif}}\geq \delta$ alors il existe un
algorithme D qui fait $q.n$ queries, qui s'exécute en temps $O(t)$ et tel
que : 
$$|Pr[D^{A_{s,\epsilon}(1^k)=1}] - Pr[D^{U_{k+1}}(1^k)=1]| \geq
(\frac{\delta+\delta_{\epsilon,l,u,n}^{\star}}{2})^3 - \frac{2^n}{2^k}
-2^{-n}.\sum_{i=0}^{2.u} \binom{n}{i} $$
\end{theo}

\begin{proof}
Étant donné un accès à un oracle qui donne des $(b,z)$ on procède comme
suit :
\begin{enumerate}
\item D choisit $s_2$ uniformément au hasard. Ensuite il exécute la
première phase de l'attaquant. Pour simuler cette étape d'authentification,
il récupère un sample $(b,z)$ de son oracle, et envoit $b$ comme message
initial. $\mathcal{A}$ répond au challenge avec un $a$, et alors D
réponds avec $z=\bar{z}\oplus (s_2.a)$. 
\item Quand $\mathcal{A}$ est prêt pour la deuxième phase du protocole.
$\mathcal{A}$ envoit un message $b_1,\dots,b_n$. Alors D choisit au
hasard $a_1^1,\dots,a_n^1$ et les envoit à l'attaquant. Il récupère
$z_1^1,\dots,z_n^1$. Puis il recommence avec $a_1^2,\dots,a_n^2$ et
$z_1^2,\dots,z_n^2$, \textsc{sans changer les $b_i$}, donc il sauvegarde
l'état de l'attaquant, ce qui est possible puisque moralement l'attaquant
c'est un programme.
\item Soit $z_i^{\oplus}=z_i^1\oplus z_i^2$ et
$Z^{\oplus}=(z_1^{\oplus},\dots,z_n^{\oplus})$. Soit
$\hat{a}_i=a_i^1 \oplus a_i^2$, $\hat{z}_i=s_2.\hat{a}_i$
et $\hat{Z}=(\hat{z}_1,\dots, \hat{z}_n)$. D renvoit 1 si et seulement si
$Z^{\oplus}$ et $\hat{Z}$ diffèrent d'au plus $2.u$ entrées.
\end{enumerate}

\emph{Correction de l'algorithme:}\\
\begin{itemize}
\item \emph{Premier cas :} Disons que l'oracle est un oracle uniforme.
Tout le monde est alors uniforme, il se passe donc que $D$ renvoit $1$
avec probabilité au plus $$2^{-n}.\sum_{i=0}^{2u}\binom{n}{i} +
\frac{2^n}{2^k} $$
Le second terme vient du fait que pour avoir le $\hat{Z}$ uniforme, il
faut que les $\hat{a}_i$ soient linéairement indépendents. Ce qui arrive
avec probabilité $2^{n-k}$.
\item \emph{Second cas :} Si l'oracle est un vrai oracle de LPN :
$A_{s_1,\epsilon}$ pour un $s_1$ aléatoirement choisit, alors D génère
une simulation parfaite de la première phase de l'attaquant. Par un
argument d'espérance,\todo[inline]{Préciser} avec
probabilité au moins
$\hat{\delta}=\frac{\delta+\delta^{\star}_{\epsilon,l,u,n}}{2}$ la
probabilité que $\mathcal{A}$ réussisse à faire une fausse
authentification est d'au moins $\hat{\delta}$. Supposons qu'on soit dans
ce cas. Alors la probabilité que $\mathcal{A}$ réussise à répondre à la
fois aux queries $a_1^1,\dots,a_n^1$ et $a_1^2,\dots,a_n^2$ est d'au
moins $\hat{\delta}^2$(Les queries sont indépendantes). Et donc ça veut
dire que $(z_1^1,\dots,z_n^1)$ et $(z_1^2,\dots,z_n^2)$  diffèrent d'au plus u entrées de la réponse
correcte.
Mais alors $Z^{\oplus}$ diffère d'au plus $2u$ entrées de la somme des
deux réponses correctes : $\hat{Z}$. Et donc on trouve la probabilité de
$\hat{\delta}^3$ voulue.
\end{itemize}
\end{proof}

\subsubsection{Man-in-the-middle secure authentication scheme}
Recemment, un protocole securisé dans le modèle man-in-the-middle
\footnote{HB\# est sécurisé seulement dans un man-in-the-middle
restreint} a été
proposé dans \cite{Lyu}. Le protocole n'est pas intrinsèquement basé sur le LPN, le LPN
n'ai qu'une instanciation possible d'un schéma générale, basé sur des
familles de fonctions \emph{Weak Pseudo Random Functions}.
\begin{defi}
Soit $\mathbb{F}$ un corps fini, et $\mathbb{D}$ un domaine, par exemple
$GF(2)^n$.
Une famille de fonction $\mathcal{H}: \mathbb{D}\rightarrow \mathbb{F}$ est dite indépendante par paires si 
$$\forall x_1 \not= x_2\in \mathbb{D}, y_1, y_2 \in \mathbb{F},
Pr_\mathcal{H}[h(x_1)=y_1 \wedge h(x_2)=y_2]=\frac{1}{|\mathbb{F}|^2}$$
\end{defi}

\begin{defi}
Une famille $\mathcal{F}$ de fonctions de
$\mathbb{D}\rightarrow\mathbb{F}$ est dite être une famille weak-PRF si
pour n'importe quel $k$ de taille polynomial, f choisie aléatoirement, et
$r_1,\dots,r_k\in\mathbb{D}$ choisis aléatoirement, la distribution de
$(r_1,f(r_1)),\dots,(r_k,f(r_k))$ est computationnellement indistinguable
de la distribution uniforme que $(\mathbb{D},\mathbb{F})^k$.
\end{defi}

En pratique on peut obtenir des familles de fonctions de ce type en rajoutant des
bruits à des fonctions, et c'est plutôt ce type d'instanciation que nous
utiliserons pour le LPN :
\begin{defi}
Soit $f$ une fonction des même domaines que précèdemment. Soit $\chi$ une
distribution sur $\mathbb{F}$. On écrira $r\mapsto f^\chi(r)$ pour désigner la
fonction randomisée qui génère un élément $e\in\mathbb{F}$ selon la loi
$\chi$ et renvoit $f(r)+e$. Une famille de fonctions est dite \emph{randomized
weak-PRF} si c'est $(r_1,f^\chi(r_1)),\dots,(r_k,f^\chi(r_k))$ qui est
computationnellement indistinguable de la distribution uniforme.
\end{defi}


\begin{figure}[h]
\centering
\begin{tikzpicture}
\draw (-1.2,2) node[above] {$\mathcal{F}$ une famille \emph{randomized
weak-PRF}};
\draw (-1,2.5) node[above] {$\mathcal{H}$ une famille indépendante par
paires};
\draw (-1.4,1.5) node[above] {Clefs : $f\in\mathcal{F},h\in\mathcal{H},
s\in \mathbb{F}$};
\draw (-0.5,1) node[above] {Tag};
\draw (2.5,1) node[above] {Reader};
 \draw[->] (0,0.25) -- (2,0.25);
 \draw[<-] (0,-0.25) -- (2,-0.25);
\draw[->] (0,-1) -- (2,-1);
\draw (1,0.25) node[above] {$r$};
\draw (1,-0.25) node[above] {$c$};
\draw (1,-1) node[above] {$z$};
\draw (-1,0.25) node {$r\leftarrow \mathbb{D}$};
\draw (3,0) node {$c\leftarrow \mathbb{F}$};
\draw (-1.75,-1) node {$z:=h(r).c + f^{\chi}(r).s $};
\draw (5.2,-1) node {Accepter si $||f(r)-s^{-1}(z-h(r).c)||\leq \tau'$};
\end{tikzpicture}
\caption{Protocole basé sur une famille de fonctions weak-PRF}
\label{protmitm}
\end{figure}

Dans le cadre du LPN, on peut instancier ce protocole de la manière
suivante :

\begin{defi}
Soit $\mathcal{F}: GF(2)^n\rightarrow GF(2)^m$ une famille de fonctions
indexées par les matrices $S\in GF(2)^{m\times n}$. Pour une fonction
$f_S\in\mathcal{F}$ et un vecteur $r\in GF(2)^n$, on définit $f_S(r):=
S.r$. Alors l'hypothèse du LPN difficile implique que $\mathcal{F}$ est
une \emph{randomized weak-PRF} famille avec un bruit $Ber_\tau$. En
effet, c'est la formalisation de l'hypothèse du LPN difficile vue à
travers le \emph{lemme technique}~\ref{lemmetechnique}.
\end{defi}

Dans \cite{Lyu}, les auteurs proposent d'autres instantiations comme le
Toeplitz-LPN et le Ring-LPN, dont les noms sont explicites. On ne detaillera pas ces sous problèmes.
L'idée consiste simplement à rajouter une structure supplémentaire pour
faciliter les implémentations, sans rendre le problème plus facile.

\paragraph{Sécurisé dans le modèle man-in-the-middle}
\begin{defi}
Le modèle d'attaque man-in-the-middle est le modèle de la
figure~\ref{mitm}.
C'est le plus fort type d'adversaire : dans la première phase il peut
interagir avec le Tag et le Reader et faire des queries de vérification
au Reader. Dans la deuxième phase il perd accès au Tag et tente de faire
une fausse authentification au Reader. 

\begin{figure}[h]
\centering
\begin{tikzpicture}
\draw (0,1) node[above] {Tag};
\draw (2,1) node[above] {Adversaire};
\draw (4,1) node[above] {Reader};
\draw[->] (0.5,0) -- (2.,0.);
\draw[<-] (0.5,-0.5) -- (2,-0.5);
\draw[->] (0.5,-1) -- (2,-1);
\draw[->] (2.5,0) -- (4,0);
\draw[<-] (2.5,-0.5) -- (4,-0.5);
\draw[->] (2.5,-1) -- (4,-1);
\draw[<-] (2.5,-1.75) -- (4,-1.75) ;

\draw (1.25,0) node[above] {$r_i$};
\draw (1.25,-0.5) node[above] {$c_i+c'_i$};
\draw (1.25,-1) node[above] {$z_i$};
\draw (3.25,0) node[above] {$r_i+r'i$};
\draw (3.25,-0.5) node[above] {$c_i$};
\draw (3.25,-1) node[above] {$z_i+z'_i$};
\draw (3.25, -1.75) node[above] {$reply_i=accept/reject$};

\end{tikzpicture}
\caption{Modèle MITM}
\label{mitm}
\end{figure}

\end{defi}
\begin{theo}
Supposons que le protocole précèdent ait la complétude $\kappa$ et qu'il y a un
adversaire qui réussisse à s'authentifier via ce schéma avec probabilité $\epsilon$
en faisant au plus $q_b$ queries préalables.
Alors il existe un algorithme,de complexité temporelle identique à
celle de l'attaquant, qui contredit l'hypothèse
d'indistinguabilité de la famille \emph{weak-PRF} avec un avantage
$$\frac{1}{2}+
\frac{1}{2}.(\kappa^{q_v-1}(\frac{\epsilon}{q_v}-\frac{1}{|\mathbb{F}|})^2-\frac{|\beta(2\tau')|}{|\mathbb{F}|})
$$
Où $\beta(l)=\{z\in \mathbb{F} : ||z|| \leq l\}$
\end{theo}
\begin{proof}

On considère  la $i\leq q_v$-ième query,  la première query pour laquelle
l'attaquant fait une fausse authentification :
$(r'_i,c'_i,z'_i)\not=(0,0,0)$. Le but est d'utiliser l'adversaire qui
casse le schéma avec la $i$-ème query pour décider quel distribution le
challenger a choisit. 

\end{proof}
\paragraph{Une attaque concurrente}
Le protocole lorsqu'il est instancié par le LPN, est cependant sensible
à des attaques concurrentes. En effet, la famille de fonctions possède
des propriétés homomorphiques : $f(r_1+r_2)=f(r_1)+f(r_2)$, si on suppose
que les fonctions dans $\mathcal{H}$ sont de la forme $h(r)=s_1.r+s_2$  Alors
l'adversaire peut opérer comme dans la figure~\ref{concur}.
\begin{figure}[h]
\centering
\begin{tikzpicture}
\draw (0,1) node[above] {$Tag_1$ et $Tag_2$};
\draw (2,1) node[above] {Adversaire};
\draw (4,1) node[above] {Reader};
\draw[->] (-0.5,0) -- (2.,0.);
\draw[<-] (-0.5,-0.5) -- (2,-0.5);
\draw[->] (-0.5,-1) -- (2,-1);
\draw[->] (2.5,0) -- (5,0);
\draw[<-] (2.5,-0.5) -- (5,-0.5);
\draw[->] (2.5,-1) -- (5,-1);

\draw (0.78,0) node[above] {$r_{1,2}$};
\draw (0.75,-0.5) node[above] {$r_1.c.r^{-1},c_1+c$};
\draw (0.75,-1) node[above] {$z_{1,2}$};
\draw (3.75,0) node[above] {$r=r_1+r_2$};
\draw (3.75,-0.5) node[above] {$c$};
\draw (3.75,-1) node[above] {$z_1+z_2$};

\end{tikzpicture}
\caption{Attaque concurrente}
\label{concur}
\end{figure}
Le fait d'avoir deux tags simultanés permet d'utiliser la propriété
suivante : Si l'erreur $e_1+e_2$ est petite (où les $e_i$ sont cachés
dans les $z_i$), alors $z_1+z_2$ pourra être une réponse valide : le
Reader accepte l'authentification si :
$||f(r)-s^{-1}.((z_1+z_2)-(s_1.r+s_2).c)||\leq \tau'$ c'est-à-dire :
\begin{tabular}{r c l}
 $|| f(r)-s^{-1}.((z_1+z_2)-(s_1.r+s_2).c)||$ & $=$ &
$||f(r)-(f(r_1)+f(r_2))-(e_1+e_2)-$\\&&$s^{-1}(s_2.(c_1+c_2)+s_1.(r_1.c_1+r_2.c_2)-(s_1.r+s_2).c)|| $ \\
 &$=$&$||-(e_1+e_2)-s^{-1}.(s_1(r_1.c_1+(r-r_1)(c_1+c))-s_1.r.c)||$ \\
 &$=$& $||-(e_1+e_2)-s^{-1}s_1(r.r_1.c.r^{-1}-r_1.c) || $\\
 &$=$& $||e_1+e_2 || $\\
\end{tabular}

Ainsi l'attaquant a pu faire une fausse authentification, et la sécurité
est compromise. 

\paragraph{TODO: Réfléchir à une proposition de correction}
\todo[inline]{A voir}

\subsection{Authentification clef publique : contribution}
De manière générale, lorsque l'on dispose d'un schéma de chiffrement à
clef publique (ou privé d'ailleurs), on peut en déduire un schéma
d'authentification selon la méthode suivante :
\begin{enumerate}
\item Le Tag fait une demande de challenge au Reader
\item Le Reader génère $M$ aléatoire, et le chiffre avec la clef privée du
Tag : $C_1$
\item Le Tag déchiffre $C_1$ avec sa clef publique, et le chiffre avec la
clef privée du Reader : il obtient ainsi $C_2$ qu'il envoit au Reader
\item Le Reader accept si et seulement si le décodé de $C_2$ est égal à
$M$.
\end{enumerate}

Cette stratégie très générale permet donc de faire de l'authentification
à clef privée. Cependant si l'attaquant est MITM, il obtient un accès à
autant de couples (clair,chiffré) qu'il veut. Pour supprimer ce défaut,
il faut que le Reader envoit un message aléatoire \emph{signé}.




\section{Modèles et cryptanalyses}
Les protocoles présentés précédemment font tous l'hypothèse que le
problème du LPN est difficile. Pour vérifier la solidité de cette
hypothèse,   nous allons
présenter rapidement les deux grands modèles d'apprentissage SQ et PAC, ainsi que le
résultat majeur de BKW de difficulté du LPN dans le modèle SQ. Ensuite
nous verrons les algorithmes classiques, des paramètres caractéristiques,
et des idées d'algorithmes ou d'améliorations testées durant le stage.

\subsection{Modèles}
Cette partie est inspirée des cours d'Avrim
Blum à la Carnegie Mellon University
\subsubsection{Modèle PAC}
Le modèle PAC, pour Probably Approximately Correct, est un modèle
d'apprentissage ici présenté dans le cadre de la
classification : pour le LPN, la classe d'un vecteur $a$ est
$a.s$.
\begin{defi}
On dit qu'une classe de fonction $\mathcal{C}$ booléennes de l'hypercube
de dimension n est PAC-learnable si pour tout $i\in\mathcal{C}$, étant donné un oracle qui génère des
entrées
selon une loi $\mathcal{D}$ et qui donne une classification correcte avec une
certaine probabilité $1-\eta$, $\forall
\delta<\frac{1}{2}, \epsilon<\frac{1}{2}$ il existe un algorithme
$\mathcal{A}$ tel qu'avec probabilité au moins $1-\delta$ l'algorithme donne une
$\epsilon$-approximation de la fonction cible sur $\mathcal{D}$. 
\end{defi}
Ce modèle est un peu le modèle canonique, celui auquel on pense
naturellement si on se pose la question de définir la notion d'être
appréhensible.
\subsubsection{Modèle SQ}
Le modèle SQ (pour Statistical Queries) propose un cadre pour étudier
certains problèmes d'apprentissages. Le modèle se définit comme une
interface restreinte entre l'algorithme d'apprentissage et la source de
données.
\\
\\
On dispose d'un oracle pouvant répondre à des questions de type "Quelle
est la probabilité que l'instance x vérifie le prédicat P". L'oracle
répond un intervalle de probabilité : c'est-à-dire "C'est entre
$p - \tau$ et $p + \tau$". La question posée doit être évaluable
en temps polynomial.
\\
\begin{defi}
On dit qu'une classe de fonction $\mathcal{C}$ booléennes de l'hypercube
de dimension n est SQ-learnable sur la
distribution $\mathcal{D}$ s'il y a un algorithme $\mathcal{A}$
tel que $\forall c \in \mathcal{C}, \epsilon>0$, $\mathcal{A}$ donne une
$\epsilon$-approximation de c à partir de l'oracle SQ. Le temps
d'exécution d'algorithme, le nombre de queries et l'inverse de la plus
petite tolérance $\tau$ doivent être polynomiaux en $n$ et en
$\frac{1}{\epsilon}$. 
\end{defi}
\begin{defi}
On dit qu'une classe de fonction est faiblement SQ-learnable, s'il existe
un $\epsilon<\frac{1}{2}-\frac{1}{poly(n)}$ tel que la définition
précédente soit vérifiée.(Quantification existentielle au lieu
d'universelle sur $\epsilon$) 
\end{defi}
Ce modèle, est en fait expressif, de nombreux problèmes sont
"SQ-learnable" comme on va le voir immédiatement.
\paragraph{Exemple de problèmes SQ-learnable}
Soit $f$ une fonction de l'ensemble des fonctions qui sont des
disjonctions de n variables booléennes. Par exemple
$f(x)=x_1 \vee x_4 \vee \dots \vee x_{n-2}$.
On cherche à l'apprendre. Pour ça on va faire les queries
$P(f(x)=0\wedge x_i=1)$ pour tout $i\in [1,n]$. Si l'i-ème réponse est
suffisamment petite : plus petite que $\frac{\epsilon}{n}$, alors $x_i$
n'est pas dans l'expression de f.  
\paragraph{SQ-learnable $\Rightarrow$ PAC-learnable}
Étant donné un nombre important de samples, on peut simuler l'oracle. Il
faut que la taille des samples soit grossièrement en
$O(\frac{1}{\tau^2})$, pour obtenir une précision de $\tau$.
  
\paragraph{Théorème de non SQ-learnabilité}
\cite{Blum} propose une caractérisation des problèmes SQ-learnable, que
l'on présente ici.
\begin{defi}
Soit $\mathcal{F}$ une classe de fonctions booléennes sur $GF(2)^n$, et $D$ une
distribution sur $GF(2)^n$.
On appelle dimension SQ de $\mathcal{F}$ le cardinal
du plus grand ensemble de fonctions $f_1,\dots, f_d$ telles que 
$$\forall i \not= j, |Pr_D(f_i=f_j)-Pr_D(f_i \not= f_j)|\leq \frac{1}{d^3} $$
\end{defi}
\begin{framed}
\begin{theo}
Soit $\mathcal{F}$ une classe de fonctions booléennes et $\mathcal{D}$
une distribution telle que $SQ_{dim}(\mathcal{F}, \mathcal{D})\geq d \geq
16$. Alors si toutes les queries sont faites avec tolérance
$\tau\geq\frac{1}{d^{\frac{1}{3}}}$, au moins $d^{\frac{1}{3}}/2$ queries
sont nécessaires pour apprendre $\mathcal{F}$ avec une erreur inférieur a
$\frac{1}{2}-\frac{1}{d^3}$\footnote{On ne s'occupera pas rigoureusement de l'erreur
dans la preuve voir \cite{Blum} pour plus de détails.}
\end{theo}
\begin{proof}
Tout d'abord certains lemme ne seront pas prouvés ici, essentiellement
parce qu'ils ne sont pas intrinsèquement intéressants, et que la place
est réduite, ce sont des lemmes de majoration qui font essentiellement
usage d'inégalités classiques.\\ 
Dans cette preuve les fonctions à apprendre sont considérées à valeur dans
$\{-1,1\}$. On considère aussi les statistical queries comme des
applications à valeurs dans $\{-1,1\}$, dont l'oracle nous donne
l'espérance sous la distribution D. On remarque au passage qu'il y a un
nombre fini de statistical queries.
\\
On définit le produit scalaire influencé par la distribution D par :
$$ f._{D}g=E_D[f.g]=\sum_{x\in\{0,1\}^n} D(x).f(x).g(x)$$
L'hypothèse peut ainsi se reformuler en :
$$\forall i \not= j , |f_i._{D}f_{j}|\leq\frac{1}{d^3}$$
Ce qui veut dire que les fonctions sont presques orthogonales.
On remarque aussi que $ f._{D}f=1$ pour toute fonction f (à valeur dans
$\{-1,1\}$).
\begin{lemma}
Pour $d\geq4$ les fonctions $f_1,\dots,f_d$ sont linéairements
indépendantes. (C'est le d de la dimension SQ!)
\end{lemma}
\begin{proof}
Supposons au contraire qu'il existe une relation linéaire non triviale , alors :
\\
\begin{center}
\begin{tabular}{r c l}
$0$ & $=$ & $E_D[(f_1-\sum\limits_{i \geq 2}\alpha_i.f_i)^2]$\\
&$=$& $E_D[f_1^2] - 2.\sum\limits_{i \geq 2}
\alpha_i.E_D[f_1.f_i]+\sum\limits_{i,j\geq
2} \alpha_i.\alpha_j.E_D[f_i.f_j] $ \\
&$=$& $1-2.\sum\limits_{i \geq 2} \alpha_i.E_D[f_1.f_i] + \sum\limits_{i\geq 2}
\alpha_i^2 + \sum\limits_{i,j\geq2, i\not=j} \alpha_i.\alpha_{j}.E_D[f_i.f_j] $ \\
\end{tabular}
\end{center}
Puis on minore le terme de droite grâce à la valeur absolue la plus
grande de coefficient : 
$$  1-2.\sum\limits_{i \geq 2} \alpha_i.E_D[f_1.f_i] + \sum\limits_{i\geq 2}
\alpha_i^2 + \sum\limits_{i,j\geq2, i\not=j}
\alpha_i.\alpha_{j}.E_D[f_i.f_j] \geq 1+
\alpha_{max}^2-\frac{2.\alpha_{max}}{d^2} - \frac{\alpha_{max}^2}{d}  $$
car $E_D[f_i.f_j]=f_i._{D}f_j \leq \frac {1}{d^3}$ par hypothèse. Or
notre minoration est elle même minorée par 0 par une étude de fonction
simple ($\alpha_{max} \geq 0$) 
\end{proof}

Sans perte de généralité on peut supposer que le support de D est tout
l'espace. En effet si ce n'est pas le cas, on prend une $\epsilon$-cacahuète de
masse, que l'on réparti uniformément sur les vecteurs dont la masse était
auparavant nulle. Avec une masse suffisament petite pour que cette masse
ne soit pas atteignable par un algorithme d'apprentissage polynomial.
\\\\
On applique Gram-Schmidt pour compléter $f_1,\dots,f_d$ en une base $f_1,\dots,f_d,\dots,f_{2^n}$ telle que :
 $$\forall i\geq d+1,\forall j\not=i,f_i._{D}f_j=0,f_i._{D}f_i=1  $$

Attention, cette base n'est pas orthonormée puisque les premiers vecteurs
sont seulement pseudo-orthogonaux entre eux.\\\\
 Sachant qu'une statistical querie est une fonction $g: GF(2)^n \times
\{-1,1\} \rightarrow \{-1,1\}$, on va chercher à construire une base des
statistical queries. Pour ça on définit $\tilde{D}$ sur $GF(2)^n\times
\{-1,1\}$, comme produit de $D$ et de la distribution uniforme. Puis on
étend les $f_i$, de deux manières différentes, pour les considérer comme statistical queries :

$$\begin{cases}
h_i(x,y)=y.f_i(x)\\
f_i(x,y)=f_i(x)
\end{cases}
$$

Or $(h_i)\cup(f_i)$ est une base des fonctions sur $GF(2)\times\{-1,1\}$
muni du produit scalaire $._{\tilde{D}}$. Pour vérifier ceci on peut
refaire une preuve identique à celle du lemme. Ainsi on peut décomposer
toute statistical query sur cette base : 
$$\forall g, g= \sum_{i\geq 1} \alpha_i.f_i + \sum_{i\geq1} \beta_i.h_i$$

\begin{lemma}
Si on décomposer $g$ comme précèdemment alors on pourra montrer :
$|\alpha_i|,|\beta_i|\leq 2$. 
\end{lemma} 
Puis alors on peut regarder l'espérance de g, sur une fonction $f_j$ :
\begin{center}
\begin{tabular}{r c l}
$E_{D}[g(x,f_j(x)]$ 
& $=$ &
$E_{D}[\sum\limits_{i\geq 1}\alpha_i.f_i(x)+\sum\limits_{i\geq 1}
\beta_i.h_i(x,f_j(x))]$ \\
&$=$&
 $\sum\limits_{i\geq 1}\alpha_i.E_D[f_i]+\sum\limits_{i\geq
1}\beta_i.E_D[f_i.f_j] $  \\
&$=$&
 $C+\sum\limits_{i\leq d} \beta_i.(f_{i}._{D}f_{j})  $
\end{tabular}
\end{center}
or 
$\beta_j-\frac{2}{d^2} \leq\sum\limits_{i\leq d}
\beta_i.(f_{i}._{D}f_{j})\leq \beta_j+\frac{2}{d^2} $ en utilisant le
lemme précèdant. Donc la contribution majeure de la fonction cible se fait
par le coefficient $\beta_j$, le reste étant en $O(1/d^2)$.

Et finalement on cherche à établir combien de fonctions dans
$f_1,\dots,f_d$ peuvent être éliminées par
cette statistical query g.
\begin{lemma}
On peut montrer  $\sum\limits_{i\leq d}\beta_{i}^2 \leq 2$\footnote{C'est ici que l'on a besoin de $d\geq16$}. 
\end{lemma}
Donc si la query g est faite avec une tolérance supérieur à
$1/d^{\frac{1}{3}}$, alors la fonction $f_j$ est éliminée seulement si
$\beta_j \geq 1/d^{\frac{1}{3}}$. Mais par le dernier lemme, si on note r
le nombre de fonctions pour lesquelles c'est le cas, on a :
$$r(1/d^{\frac{1}{3}})^2\leq 2$$
$$r\leq2.d^{\frac{2}{3}}$$
Donc autrement dit, au moins $d^{\frac{1}{3}}/2$ queries sont necessaires
pour éliminer les fonctions de $f_1,\dots, f_d$ qui ne sont pas la
fonction cible, puisque chaque query ne peut en éliminer qu'un nombre
inférieur à  $2.d^{\frac{2}{3}}$.

\end{proof}
\end{framed}
\begin{theo}
L'ensemble des fonctions booléennes sur $GF(2)^n$ n'est pas
SQ-learnable.
\end{theo}
\begin{proof}
En effet la dimension SQ de l'ensemble des fonctions affines booléennes sur
$GF(2)^n$ n'est pas polynomiale :
\\\\
On prend $D$ la distribution uniforme, alors 
$\forall a, a'\in GF(2)^n,
Pr_D(a.x=a'.x)=\frac{1}{2}$, $Pr_D(a.x\not=a'.x)=\frac{1}{2}$
 De plus on ne peut pas faire des queries avec
des tolérances inférieures à celles de la proposition, en temps
polynomial.
\end{proof}
En conséquence ce dernier théorème nous donne une condition nécessaire
pour trouver des algorithmes en temps sous-exponentiel : Il faut qu'ils
utilisent des "opérations non SQ-like". Par exemple, des combinaisons
linéaires.

\subsection{Algorithmes pour attaquer le LPN}
Il est nécessaire de distinguer deux grandes classes
d'algorithmes, en fonction du nombre de samples (donc d'authentification)
nécessaire. Dans un premier temps nous allons étudier les algorithmes
nécessitant peu de queries (donc plus lents), ensuite nous verrons
les algorithmes "BKW-like" nécessitants plus de queries.


\subsubsection{Avec peu de queries}
\paragraph{Recherche exhaustive}
Comme toujours en cryptographie, on peut chercher à effectuer une attaque
par recherche exhaustive. Cependant ici plusieurs types de recherches sont
possibles.
\begin{itemize}
\item On peut effectuer une recherche exhaustive naïve sur le
secret, et tester pour chaque secret si la proportion d'erreur est égale
au bruit.
\item On peut effectuer une recherche exhaustive sur le bruit, chercher
la position des erreurs, car on sait qu'il y a peu d'erreurs, ensuite
retrouver le secret associé par une élimination de Gauss, puis tester que
ce secret est en effet bon. Le problème est que l'élimination de Gauss
est assez coûteuse : $O(n^3)$.
\item \cite{Gol} fait moralement quelque chose d'équivalent, mais
présenté de manière différente, ceci facilitant implantations :
$s=A^{-1}.(\nu+b)$, et sa recherche
exhaustive sur s se fait en recherchant exhaustivement sur $\nu$ par poids
de Hamming croissant. Cette manière d'énumérer permet de tomber beaucoup
plus rapidement sur la solution : tous les mots n'ont pas la même
probabilité d'être secret. En effet, la distribution du poids de
hamming du bruit est fortement chargée autour de $\epsilon.n$. En pratique pour des bruits suffisamment
faibles\footnote{0.01 pour n=768 par exemple}, c'est la meilleure attaque. Cette attaque a
le mérite de se paralléliser très facilement. 
\end{itemize} 

\paragraph{Attaque par vecteurs corrélés}
Ce deuxième type d'attaque est asymptotiquement meilleur mais théorique.
\subparagraph{Découpage et compromis temps-mémoire de \cite{Grigo}}

\cite{Grigo} utilise en boite noire le résultat suivant :
\begin{framed}
\begin{theo}\cite{Indyk}
            \\ Étant donné N points de l'hypercube de dimension d, on peut
      trouver une paire de points $\delta$-proche, avec $\delta \leq
      \epsilon.\rho$ où $\epsilon$ est la plus petite distance entre deux
       des N points. Ceci est fait en $O(d.N^{1+\frac{1}{\rho^2}+o(1)})$
\end{theo}
\begin{intui}

L'idée consiste à hacher les vecteurs par de multiples fonctions de
hachages dites \emph{local-sensitive} : ces fonctions hachent des entrées
proches vers la même sortie. $$\begin{cases} ||p-q||\leq R \Rightarrow
Pr(h(q)=h(p))\geq P_1 \\||p-q||\geq c.R \Rightarrow Pr(h(p)=h(q))\leq P_2
\end{cases} $$ Où  $P_1 > P_2$. L'utilisation de plusieurs fonctions de
hachage simultanées permet de diminuer la probabilité que deux vecteurs
pas réellement proches se retrouvent hachés ensemble.

On cherche
alors simplement à trouver deux vecteurs dans la même entrée de la table
finale.  
\end{intui}
\end{framed}

  L'approche consiste à supposer que le secret est sparse de poids de
  Hamming r
  donné.\footnote{Plus tard on verra qu'en fait on peut $\epsilon.n$-sparsifier un
  secret, voir plus bas l'étape de réduction à un secret sparse}
  L'idée de \cite{Grigo} est de construire tous les secrets de poids de
  Hamming $\frac{r}{2}$, en construisant :
\begin{itemize}
      \item $\mathcal{H}_1$ l'ensemble des évaluations, image des vecteurs de
      poids r/2.
      \item $\mathcal{H}_2$ l'ensemble des évaluations, image des vecteurs de
      poids r/2, xorées avec le vecteur b.
\end{itemize}  
\begin{theo} Il existe un algorithme qui résout un LPN r-sparse en
    effectuant
    $\frac{r.log(\frac{n}{\delta}).\omega(1)}{\epsilon^2.(1-2.\eta)^2}$
    queries en un temps
    $$\frac{log(\frac{1}{\delta}).n^{(1+(2.\eta)^2+o(1))r/2}}{(1-2\eta)^2}$$
\end{theo}
\begin{proof} Étudions d'abord le cas ou le bruit est nul. On
  remarque que dans ce cas, si on regarde les deux bouts de secrets $c_1+c_2=c$ de poids
  r/2 chacun, leurs images dans $\mathcal{H}_1$, $\mathcal{H}_2$ sont à
  distance de Hamming 0. \\
  Dans le cas où il y a du bruit, ils sont à
  distance de Hamming valant la valeur du bruit. Ceci est caractéristique
  des composantes du secret puisqu'en position générale, on obtient des
  vecteurs distants d'environ n/2. En utilisant le théorème de \cite{Indyk}
  on peut alors retrouver ces deux parties du secret, en cherchant dans
  $\mathcal{H}_1$ et $\mathcal{H}_2$ deux vecteurs proches. La preuve de complexité
  est en temps est alors :
\begin{itemize}
\item Générer les ensembles $\mathcal{H}_1,\mathcal{H}_2$ coûte
$O(n^\frac{r}{2})$  
\item Trouver des vecteurs proches
$O(d.n^{(1+\frac{1}{\rho^2}+o(1)).\frac{r}{2}})$ où d est de l'ordre du
nombre de samples. 
\end{itemize} 
\end{proof}
Cet algorithme se comporte donc en $O(n^{\frac{k}{2}})$ lorsque le bruit
est faible, et en $O(n^k)$ lorsque le bruit tend vers 1/2.

\subparagraph{Attaque théorique de \cite{Valiant}} 

 \cite{Valiant} propose une attaque du même type, cherchant deux vecteurs
corrélés, quand les autres ne le sont pas :
\begin{framed}

\begin{theo}
$\forall \epsilon > 0,$ pour $N$ suffisamment grand, soit $N$
vecteurs dans $\{-1,1\}^k$ choisis uniformément au hasard, sauf une paire
qui possède un produit scalaire supérieur ou égal à $\rho.k$. On peut
repérer cette paire de vecteur corrélés en temps
$$O(\frac{N^{\frac{5-\omega}{4-\omega}}}{\rho^{2.\omega}})<N^{1.62}.poly(1/\rho)$$
où $\omega < 2.38$ est l'exposant de la multiplication matricielle.
\end{theo}
\begin{intui}
La stratégie est dite selon \cite{Valiant} type \emph{Expand and Aggregate}.

\begin{itemize}
\item \emph{Calcul de produit scalaire, Expand :} On place N colonnes de
dimension k dans une matrice $X$. Ce sont les vecteurs dont on cherche à
calculer le produit scalaire, pour calculer la corrélation : en effet
$d(u,v)=\frac{k-u.v}{2}$. On remarque que $W=X^t.X$ contient exactement
les corrélations qu'il nous faut. Ainsi on cherche l'élément maximal en
dehors de la diagonale qui nous donnera les deux vecteurs les plus
corrélés.
\item \emph{Aggregate :} Lorsque l'on dispose de N colonnes dont deux sont
très correlées, on peut agréger les colonnes en les regroupant par
groupe de l et en xorant les éléments d'un groupe tous ensemble pour
générer un  seul représentant. On remarque alors qu'une forte corrélation subsiste si on n'a pas
regroupé les deux entrées corrélées dans un même groupe. 
\end{itemize} 
Plutôt que d'effectuer directement le calcul de correlation (amélioré par
le produit matriciel rapide), on peut effectuer une étape d'agrégation,
par exemple en $N^{\frac{2}{3}}$ groupes de $N^{\frac{1}{3}}$ éléments.
Ainsi on retrouve les deux groupes correlés, puis on termine soit par simple
bruteforce soit on relance sur une instance plus petite.
La complexité dépend donc de la taille de la matrice
initiale (qui doit posséder suffisament de ligne pour que l'agrégation ne
fasse pas disparaitre la corrélation) et du temps passé à calculer les
produits matriciels. \cite{Valiant} détermine l'optimum pour
$k=N^{\eta+\frac{1}{4-\epsilon}}/\rho^2, \forall \eta>0$\footnote{Le $\eta$ intervient
dans la réussite avec probabilité
$1-o(1)=1-\frac{1}{N^{\eta+\frac{1}{4-\epsilon}}}$}
\end{intui}
\end{framed}

\begin{theo}
Il existe un algorithme qui résout le 2-sparse LPN de dimension n en temps
sous-quadratique en n. 
\end{theo}

\begin{proof}
On supprime toutes les équations obtenues dont le membre de droite vaut
1. Ainsi on obtient donc $A'.s=0_{GF(2)^l}$, qui sont l équations dont le
membre de droite vaut 0. Alors, le problème du LPN 2-sparse revient à
chercher deux colonnes de $A'$ correlées. Il y a n colonnes, en dimension
l, ce qui donne avec probabilité $1-o(1)$ la complexité majorante : 
$$n^{1.62}.poly(\frac{1}{1-2.\epsilon}) $$ 
\end{proof}

\begin{framed}

\begin{theo}\label{reductionvaliant}
Étant donné un algorithme qui résout le r-sparse LPN en dimension n en temps
$O(n^\alpha)$, on peut construire  un algorithme qui résout un
$k$-sparse $\forall k \geq r$ en temps $O(n^{\frac{k.\alpha}{r}})$
\end{theo}

\begin{proof}
Le but est de transformer les exemples initiaux du LPN $k$-sparse de taille n, en
exemples pour un LPN r-sparse de taille environ $n^{\frac{k}{r}}$.

Pour tout exemple du LPN $k$-sparse de taille $n$, on génère un exemple
en prenant pour les composante, le xor des parties de
$\frac{k}{c}$ composantes de l'exemple initial.

Si les exemples initiaux contiennent un ensemble de $k$ composante dont
les xors sont
correlés avec les labels, alors cette ensemble va s'éclater dans les
exemples générés, qui va contenir des
ensembles (éventuellement plusieurs, numérotés de 1 à 3 dans la
figure~\ref{expli}) de $c$ composantes dont les xors sont correlés avec les labels. 
Rigoureusement les exemples générés ne suivent plus une distribution
uniforme, par exemple il y a plus d'une paire correlée avec le label, cependant l'algorithme de recherche de vecteurs correlés est
suffisament stable selon \cite{Valiant}. Voir figure~\ref{expli} pour un schéma plus explicatif
que la prose.
\\\\
Finalement, on résout cette instance construite, on retrouve donc une
solution de poid de hamming $r$, qui se relève et se "diffuse", par
construction, en la solution, qui est de poid de hamming $k$. 

\end{proof}
\end{framed}


\begin{figure}[h]
\centering
\begin{tikzpicture}

\draw[color=blue] (2.25,5.5) node[above] {$\bullet$} ;
\draw[color=blue] (3.25,5.5) node[above] {$\bullet$} ;
\draw[color=blue] (4.25,5.5) node[above] {$\bullet$} ;
\draw[color=blue] (6.25,5.5) node[above] {$\bullet$} ;

\draw[color=green] (-0.75,0) node[below] {$\bullet 1$};
\draw[color=green] (7.25,0) node[below] {$\bullet 1$};

\draw[color=red] (0.25,0) node[below] {$\bullet 2$};
\draw[color=red] (5.25,0) node[below] {$\bullet 2$};

\draw[color=brown] (2.25,0) node[below] {$\bullet 3$};
\draw[color=brown] (3.25,0) node[below] {$\bullet 3$};

\draw (2,4) rectangle (2.5,5.5);
\draw (3,4) rectangle (3.5,5.5);
\draw (4,4) rectangle (4.5,5.5);
\draw (5,4) rectangle (5.5,5.5);
\draw (6,4) rectangle (6.5,5.5);

\draw (-1,0) rectangle (-0.5,1.5);
\draw (0,0) rectangle (0.5,1.5);
\draw (1,0) rectangle (1.5,1.5);
\draw (2,0) rectangle (2.5,1.5);
\draw (3,0) rectangle (3.5,1.5);
\draw (4,0) rectangle (4.5,1.5);
\draw (5,0) rectangle (5.5,1.5);
\draw (6,0) rectangle (6.5,1.5);
\draw (7,0) rectangle (7.5,1.5);
\draw (8,0) rectangle (8.5,1.5);
%1 edges
\draw[color=green] (2.25,4) -- (-0.75,1.5);
\draw[color=green]  (3.25,4) -- (-0.75,1.5);

\draw[color=red] (2.25,4) -- (0.25,1.5);
\draw[color=red] (4.25,4) -- (0.25,1.5);

\draw (2.25,4) -- (1.25,1.5);
\draw (5.25,4) -- (1.25,1.5);

\draw[color=brown] (2.25,4) -- (2.25,1.5);
\draw[color=brown] (6.25,4) -- (2.25,1.5);
%2 edges

\draw[color=brown] (3.25,4) -- (3.25,1.5);
\draw[color=brown] (4.25,4) -- (3.25,1.5);

\draw (3.25,4) -- (4.25,1.5);
\draw (5.25,4) -- (4.25,1.5);

\draw[color=red] (3.25,4) -- (5.25,1.5);
\draw[color=red] (6.25,4) -- (5.25,1.5);
%3 edges

\draw  (4.25,4) -- (6.25,1.5);
\draw  (5.25,4) -- (6.25,1.5);

\draw[color=green] (4.25,4) -- (7.25,1.5);
\draw[color=green] (6.25,4) -- (7.25,1.5);


%4 edges

\draw (5.25,4) -- (8.25,1.5);
\draw (6.25,4) -- (8.25,1.5);

\end{tikzpicture}
\caption{Cas $r=2$, $k=4$}
\label{expli}
\end{figure}

\begin{coro}
Il existe un algorithme qui résout avec probabilité $1-o(1)$ le LPN $k$-sparse en dimension n, avec
complexité :
$$O(n^{0,81.k}.poly(\frac{1}{1-2.\epsilon}))$$
\end{coro}

\begin{theo}{Version forte non prouvée}
\\
Pour tout $\eta>0$
On peut résoudre le $r$-sparse LPN de taille $n$ et de bruit $\epsilon$
avec probabilité $1-o(1)=1-\frac{1}{N^{\eta+\frac{1}{4-\epsilon}}}$ avec une complexité temporelle de :
$$O(n^{\frac{(\omega+\eta).r}{3}}.poly(\frac{1}{1-2.\epsilon}))<
O(n^{0,8.r}.poly(\frac{1}{1-2.\epsilon}))$$
\end{theo}
\begin{intui}
\cite{Valiant} utilise le théorème ~\ref{reductionvaliant} avec une instance
initiale de taille 3 au lieu de taille 2, qui permet un gain 0.02 pour le
facteur dans
l'exponentielle.
\end{intui}

L'attaque est donc mieux que l'attaque précèdente pour
des bruits plus élevés, puisque l'exposant est indépendant du bruit,
contrairement à l'attaque précèdente.
\\
Enfin, bien que cette dernière attaque fasse disparaître le bruit de
l'exposant, cette disparition est assez factice. En effet
classiquement quand on se réduit à un secret sparse, on se réduit à un
secret dont la sparsité dépend du bruit. Donc le k dépend de $\epsilon$
et le bruit réapparait dans la formule. Cette transformation en ajoutant
une variable k, et en la rendant indépendante de n est donc utile, permet
d'établir des résultats théorique intéressant, mais n'attaque pas le LPN
classique. L'attaque porte donc uniquement sur les implémentations avec
un secret k sparse, k indépendant de n. 

\paragraph{Attaque par dualité : Contribution} 

Il est assez classique que le problème du LPN soit relié à un problème de
décodage : A est la matrice génératrice d'une code linéaire, et on
cherche à déterminer quel est le mot code réel $As$ en partant du mot code
bruité $As+\nu$.
\\
 Ici nous décrivons une autre manière de voir ce
problème, pour donner un sens à la matrice noyau à gauche de A. Cette
méthode s'est trouvée être a posteriori une réduction à un décodage de
code linéaire aléatoire.
\\\\
\label{SVP} 
On remarque que lorsque l'on xor deux équations, la validité de
l'équation xor est le xor de la validité des deux équations parentes.
Alors on va construire un système sur la validité des équations : en
effectuant des combinaisons qui lient les vecteurs de A, on va se ramener
à des équations vraies : $0=0$ et des équations fausses : $0=1$, et ces
équations sont issues d'un certain nombre de combinaison linéaires
d'équations initiales, on sait lesquels. Ces combinaisons linéaires sont en
fait le noyau à gauche de la matrice A, donc on peut en avoir une base en
temps polynomial que l'on empacte en ligne dans une matrice, que l'on
appelle $A^\dagger$. Bien entendu la dimension de cet espace est n,
cependant le LPN devient maintenant : trouver une solution courte au
système  $$A^\dagger.x=b$$ Autrement dit, trouver un nombre minimum
d'équations fausses qui justifierait que les équations produites de type $0=0$ et
$1=0$ soient respectivement vraies et fausses, \\\\
Pour simplifier le problème, on peut simplement garder des équations initiales dont le b vaut 0. 
L'avantage c'est qu'ainsi on cherche un
vecteur court dans $ker(A^\dagger)$. 


\subparagraph{Decoding in $2^{n/20}$} 
Le decoding problem possède une littérature très étoffée de \cite{Stern}
à \cite{Joux}. Il consiste à trouver le mot code à partir du syndrome (en
le calculant avec le mot code bruité). Les améliorations finales dues à
Joux et May donnent une complexité en $2^{n/20}$. Nous ne nous étendrons
pas sur ces méthodes parce qu'elles relèvent d'un autre domaine classique
et que nous ne pouvons pas facilement procéder à des tests (pas de
decoding implémenté en Sage).

\subparagraph{Via réduction de réseau} 
La recherche d'un vecteur court fait penser à la réduction de réseau et
donc à LLL. On sait en temps polynomial trouver d'assez bonnes bases dont
le premier vecteur court est optimal d'après \cite{Phong} à un facteur expérimentalement
$1.08^{\frac {n-1} 2}$ près.
Cependant LLL ne fonctionne pas dans $GF(2)$\footnote{les réseaux dans un
$GF(2)$ espace vectoriel étant moralement des sous-groupes de $GF(2)^n$
donc des $GF(2)^k$, ce n'est pas très riche}. Néanmoins, remarquons que
si 
 $x\in GF(2)^n$, où l'espace est métrique pour la distance de Hamming, considéré comme un vecteur de $\mathbb{R}^n$ on a :
$$HammingWeight(x)=\| x \| ^2$$  
 Martin Albrecht propose alors qu'en pratique un LLL dans R donnera un
vecteur court dont les composantes seront en valeur absolue plus petites
que 1, que l'on pourra ensuite re-projeter sur $GF(2)$ qui donnera un bon
vecteur court.
\\
On remarque dans tous les cas que $1.08^{\frac {n-1} {2}}$ devient
rapidement trop grand, et donc qu'LLL n'assure plus du tout de trouver un
vecteur optimal. Voir~\ref{Impsvp} pour des ordres de grandeurs.

\paragraph{Changement de modèle}

 Dans \cite{Arora} les auteurs constatent
que lorsque l'on change légèrement le modèle, on peut trouver des
solutions en temps polynomiales au problème du LPN ainsi modifié. Plutôt que de tirer les samples au coup par coup,
ils proposent d'avoir un "bouton", qui permet d'obtenir m samples dont
\emph{on
est sûr qu'une fraction $\tau$ est bonne}.


\begin{framed}
\begin{defi}
On appelle oracle à motif, un oracle $O_P$, où $P$ est un polynôme en m
variables. Lorsqu'on questionne l'oracle, il donne $a_1, \dots, a_m$,
ainsi que $a_i.s+\nu_i$ où s est le secret, et $\nu$ est un vecteur tel
que $P(\nu)=0$, où chaque composante de $\nu$ est une variable du
polynôme. Autrement dit, l'oracle à motif donne des bruits qui sont
parmis les racines du polynôme.
\end{defi}

\begin{theo}
Soit $P$ un polynome de m variables tel qu'au moins un $\rho\in GF(2)^m$
ne s'écrive pas sous la forme $\rho=\nu + \nu'$ où $P(\nu)=P_(\nu')=0$.
Alors il existe un algorithme qui apprend le secret s en temps
$poly(n^m)$.
\end{theo}

\begin{intui}
On a $P(\nu_1,\dots,\nu_m)=0$, on fait le changement de variable
$\nu_i=a_i.s+b_i$ donc on a 
$$P(a_1.s+b_1, \dots, a_m.s+b_m)=0$$
Donc ça nous fait des équations polynomiales en s (on peut répéter
plusieurs fois l'opération). Ensuite on les
linéarise, en remplaçant les monômes présents par des nouvelles
variables. Le polynôme est par définition d'un degrès d connu, donc on a
$N=\sum_{i=1}^d \binom{n}{i}$ variables.
\\\\
\cite{Arora} propose qu'en prenant $10.N.2^{m+d}$ samples, le système
aura au moins une solution, et avec grande probabilité, toutes les
solutions du systèmes construit seront solutions du LPN initial.
Le fait de poser un polynôme intermédiaire, dont le degrès dépend en fait
normalement
de la dimension, éclate la complexité en la paramétrant, et elle devient
polynomiale en la dimension. L'article cible donc où se situe la
difficulté du problème de manière plus précise : dans la forme du bruit. 
\end{intui}
 
\begin{coro}
En prenant un polynôme P tel que $P(\nu)=0$ si et seulement si $\nu$ a un
poids de Hamming d'au plus $m/3$. C'est un polynôme, par interpolation de
Lagrange. On obtient alors un algorithme
polynomial : $O(n^m)$
\end{coro}
\end{framed}



\paragraph{Réduction de bruit : Contribution}  \label{SAT} 
Jusqu'à présent nous n'avons pas encore vu de méthodes pour réduire le
bruit du LPN. On commence par remarquer que l'opération booléenne OU peut
se traduire dans $GF(2)$ par les lois de De Morgan de la façon suivante : 
$$x\vee y=1-(1-x).(1-y)$$ 
Alors on peut faire le OU de disons k équations, et
donc on obtient une équation juste avec très grande probabilité.
Ainsi en augmentant le degré des équations, on diminue le bruit à
vitesse exponentielle :
 $$P(\bar\nu_1 \vee \bar\nu_2 \vee \dots \vee \bar\nu_k)=(\epsilon)^k$$
\\\\
Cette stratégie génère donc un système d'équations polynomiales. Pour le
résoudre on peut envisager plusieurs solutions :
\begin{itemize}
\item Bases de Grobner
\item Relinéarisation : on pose tous les monômes présents comme des
variables
\item SAT-solving
\end{itemize}
Voir~\ref{ISAT} pour implantations et les résultats.

\subsubsection{Approche "légo"}
Il s'avère que les algorithmes  nécessitant un grand nombre
de queries sont tous construits autour des mêmes idées. Dans le but de
montrer cette unité, dans cette partie nous allons présenter différents
blocs de bases, extraits d'algorithmes divers. Ensuite nous décrirons les
différentes attaques comme des constructions avec ces blocs. Enfin nous
présenterons notre propre vision.  

\paragraph{Réduction de la taille du problème}
Le premier bloc consiste en la réduction de la taille du problème, au
prix d'une augmentation très significative du bruit.
\\
Pour cela, on prend nos samples, que l'on classe par groupes : sont dans
le même groupe tous les éléments qui ont en communs les $b$ premiers bits.
Ensuite dans chaque groupe on additionne un élément à tous les autres et
on supprime cet élément, voir figure~\ref{bbkw}. Ainsi tous les éléments ont leurs $b$ premiers
bits à zéro. On recommence avec les $b$ suivant etc\dots
On répète cette opération a fois. Et on a ainsi réduit le problème à une
taille $n-ab$. En pratique pour faire les groupes on effectue simplement
un tri sur les bits concernés.
\begin{figure}
$$\begin{pmatrix}
\begin{cases}00\dots01 \\00\dots01\\ \vdots\\00\dots01 \end{cases} & \star \\
\begin{cases} 00\dots10 \\00\dots10\\ \vdots\\00\dots10  \end{cases} & \star  \\
\vdots & \vdots \\
\begin{cases} 11\dots11 \\11\dots11\\ \vdots\\11\dots11  \end{cases} & \star 
\end{pmatrix}$$
\caption{Réduction de taille par groupements, 1ère étape}
\label{bbkw}
\end{figure}
\\
\\
\\
\cite{BKW} à l'origine de cette idée montrent qu'alors le biais final est
de $(1-2\epsilon)^{2^a}$ et que la complexité en temps est de
$a(a2^b+\frac {n} {(1-2\epsilon)^{2^a}})$.

\paragraph{Réduction à un secret sparse.}
\cite{Kirchner}, \cite{Bernstein} remarquent que le problème du LPN peut
se réduire en temps polynomial a un problème du LPN avec un secret sparse
 par la suite d'opérations suivantes:
$$
\begin{cases}
A_1s+\nu_1=b_1\\
A_2s+\nu_2=b_1\\
\end{cases}
$$
$$
\begin{cases}
s=A_1^{-1}(b_1+\nu_1)\\
A_2A_1^{-1}\nu_1+\nu_2=b_2+A_2A_1^{-1}\nu_1
\end{cases}
$$

\paragraph{Stratégie de résolution finale}
\subparagraph{Vote}
On peut tout simplement espérer avoir les vecteurs de la base canonique
en ligne dans notre matrice, et espérer qu'on les a tous un nombre
suffisant de fois,
pour ensuite voter à la majorité, la valeur du bit de secret
correspondant. Il faut donc que le problème soit de petite dimension, et
qu'on ait beaucoup de samples. 
\subparagraph{FFT}
\begin{framed}
On sait que la transformée de Fourier de notre fonction, qu'on appelle
aussi transformée de Walsh-Hadamard, permet de trouver la meilleure
approximation affine d'une fonction booléenne.\\\\
 En effet : 
\begin{theo}
Les caractères irréductibles sont les $$\chi_y(x)=(-1)^{x.y},
\forall y \in GF(2)^n.$$
\end{theo}
\begin{theo}
Soit $\phi_y(x)=y.x=x_1.y_1+\dots+x_n.y_n$, $d$ la distance
de Hamming, $\mathcal{W}$ la transformée de Walsh, et f une fonction
booléenne. Alors
$$d(f, \phi_y)=2^{n-1}-\frac{1}{2}.max\{\mid\mathcal{W}(x\mapsto(-1)^{f(x)})(y)\mid| 
y\in GF(2)^n \}$$
\end{theo}
\begin{proof}
On a : \\\\
\begin{tabular}{r c l}
$\mathcal{W}(x\mapsto(-1)^{f(x)})(y)$&$=$&$\sum\limits_{x\in GF(2)^n}(-1)^{f(x)+x.y}$\\
&$=$&$card\{x\in GF(2)^n | f(x)=x.y\}-card\{x\in GF(2)^n | f(x)\not=x.y\}$\\
&$=$&$2^n-2d(f, \phi_y)$
\end{tabular}
\end{proof}
\end{framed}

Donc pour connaître la meilleure approximation affine, il suffit de connaître la position du coefficient de la transformée
le plus grand. Or la meilleure approximation affine, c'est exactement le
secret.
 Cette procédure a été proposée par \cite{LF}.
En pratique le calcul d'une transformée de Walsh-Hadamard coûte $2^n.n$,
cependant \cite{Kirchner} propose de gagner un peu si le secret est
sparse. 
 
\subparagraph{Approche SFT}
\label{SFT}
Des algorithmes pour connaître les coefficients de Fourier dominant d'une fonction d'un groupe
abélien ont été proposés par \cite{Mansour} (pour les fonctions booléennes)
puis  \cite{Akavia}.
\\
\cite{Vaudenay} décrit précisément le fonctionnement de
l'algorithme, qui est une recherche dichotomique par le biais d'un
estimateur, que nous appellerons estimateur AKMV. Cet estimateur AKMV
permet d'estimer la somme des coefficients de Fourier qui sont dans un
voisinage (dont on peut contrôler la taille) d'un point donné. Ainsi
l'algorithme de SFT consiste simplement à faire une recherche
dichotomique, en éliminant à chaque fois les branches ne contenant pas un
poids en coefficient de Fourier suffisant.
\begin{defi}
 En dimension n, l'estimateur akmv en h de voisinage k vaut
$$akmv(h, k)=\frac{2^{2.n}}{N}.\sum_{i=0}^{n}
f(a_i+b_i).f(a_i+c_i).(-1)^{(b_i+c_i).h}$$
où 
\begin{itemize}
\item $a_i$ sont des mots quelconques de n bits.
\item $N$ est indépendante de la dimension, permet de dimensionner la
finesse d'évaluation des coefficients.
\item $c_i, b_i$ sont des mots quelconques de n bits possédants leur k derniers bits à
0.
\end{itemize}
\end{defi}

\begin{theo}
$akmv(c, \delta)$ est un bon estimateur de la somme des carrés modules des coefficients
de Fourier :
$$\sum_{\alpha\in I(c, \delta)} |\hat{f}(\alpha)|^2 - \gamma.2^{2.k}\leq
2^{2.k}.akmv(c, \delta)\leq
\sum_{\alpha\in I(c, \delta)} |\hat{f}(\alpha)|^2 +  \gamma.2^{2.k}$$
\begin{itemize}
\item $I(c, \delta)$ est l'ensemble des points qui possèdent les $\delta$
premières coordonnées de c, et des coordonnées suivantes quelconques.
\item $\gamma=\frac{L_2(\hat{f})^2}{18.2.2^{2.k}}$
\item Probabilité d'échec plus petite que $2.\epsilon'$ 
\item $N=2.\gamma^{-2}.ln(\frac{1}{\epsilon'})$
\item Erreur bornée par $\frac{1}{18.2}.L_2(\hat{f})^2$ 
\end{itemize} 
\end{theo}


Dans le LPN on ne peut pas évaluer notre fonction n'importe où, donc on
ne peut pas effectuer l'algorithme de SFT \cite{Akavia}. Cependant on peut calculer
l'estimateur AKMV pour tous les points, pour un voisinage réduit au
centre.  Autrement dit on peut évaluer n'importe quel coefficient de
Fourier en $O(n)$. Donc pour nous : on peut tester un secret en $O(n)$.
Ce qui est mieux que le $O(n^2)$ traditionnel. \\
En fait on pourra critiquer aisément le fait que cette amélioration est
asymptotique, en pratique il y a un facteur entre $1000$ et $10000$ dans
le O\footnote{Selon la précision désirée dans l'approximation du
coefficient de Fourier}. 
\subsubsection{BKW et LF}
\begin{itemize}
\item \cite{BKW} part avec $2^\frac{n}{log(n)}$ samples, et effectue une
réduction de taille avec $a=log(n)$, $b = \frac{n}{log(n)}$.
Ensuite il fait un vote à la majorité. La complexité
est de $O(2^{O(\frac {n} {log(n)})})$   
\item \cite{LF} commence de la même manière et termine par une
transformation de Walsh-Hadamard. La complexité est de
$O(2^\frac{n}{log(n)})$.
\end{itemize}
\subsubsection{Kirchner et Bernstein}
\cite{Bernstein}, \cite{Kirchner} proposent de faire toutes les étapes, et
choisir les paramètres de manières optimale : vu qu'ils gagnent un peu
dans la FFT lorsque le secret est très sparse, ils attendent que le
secret soit très sparse (par exemple moins de $4$ bits à $1$). Et pour
chaque essaie, ils effectuent quelques étapes de BKW (Pour ordre de
grandeur, Kirchner propose
pour $n=768$, $\epsilon=0.05$ de prendre $a=3$, $b=8$.)
\\Le principal avantage de cette approche est la diminution de la
quantité mémoire nécessaire. Le nombre de queries ne diminue pas
beaucoup, mais le plongement dans une étape de randomisation permet
d'économiser beaucoup de mémoire. C'est une amélioration significative
puisque c'était le principal bottleneck dans BKW, où la mémoire coûtait
aussi en $O(2^\frac{n}{log(n)})$ 
\\\\
Dans \cite{Bernstein}, ils proposent de rajouter une structure
d'anneau : le ring-LPN. Ceci permettant de diminuer le coup des produits
matriciels.  

\subsubsection{Amélioration asymptotique par SFT : Contribution}
Si on garde la même trame que \cite{Kirchner} et \cite{Bernstein}, mais que l'on utilise notre
capacité à tester un secret en $O(n)$ vu dans~\ref{SFT}, on peut alors faire une phase
finale qui consiste à énumérer tous les mots de poids de Hamming faible (les plus
probables), et de les tester. Ainsi, si le secret possède $w$ bits à $1$
dans ses $n-ab$ derniers bits,
la complexité de la phase finale est de $\binom{n-ab}{w}.n$, \footnote{Comme dans
\cite{LF}, \cite{BKW}, \cite{Kirchner}, \cite{Bernstein}, on ne s'attarde
pas à établir la complexité pour retrouver les $a.b$ premiers bits.}

\paragraph{Optimisation des paramètres de l'algorithme précèdent.}
Donc si on prend notre dernier algorithme, sa complexité générale est
la suivante :
$$\underbrace{(\sum_{i=0}^{w}\binom{n-a.b}{i}.\epsilon^i.(1-\epsilon)^{n-a.b-i})^{-1}}_{\text{Attente
de secret w sparse}}.
(\underbrace{n^3}_{\text{Réduction sparse}} +
\underbrace{a.(a.2^b+\frac{n-a.b}{(1-2.\epsilon)^{2^a}})
}_{\text{Réduction BKW}} + \underbrace{\sum_{i=0}^w \binom{n-a.b}{i}.(n-a.b)}_{\text{Retrouver solution par SFT}} )$$ 
Cette complexité est en fait simplifiée : elle est donnée ici en O, alors
que normalement il y a des constantes qui dépendent très fortement de
l'implémentation\footnote{Notamment des facteurs 1/32 ou 1/64 par
endroit si on fait des représentations compactes avec des entiers}. On sent
donc bien qu'il faut optimiser en a, b, w, à $\epsilon$ et n fixés. Pour
cela une étude analytique semble très délicate\footnote{Surtout qu'en
crypto les paramètres sont assez petits donc il ne fait aucun sens de
faire une étude asymptotique}. En pratique, l'espace des paramètres à
optimiser étant fini un petit script OCaml permet de donner les valeurs
optimales. 
\section{Implantations}
\subsection{C}
\subsubsection{Éléments de bases pour LPN}
J'ai programmé en C un ensemble de fonctions pour pouvoir effectuer des
tests sur le LPN. L'implémentation est compacte : je stocke dans un
entier non-signé 32 valeurs binaires. Cette représentation compacte était
faite en prévision du coût mémoire des attaques BKW-like. Basé sur cette représentation, j'ai
programmé l'ensemble des outils de bases d'algèbre linéaire nécessaires
simples :
\\
\begin{itemize}
\item Produit scalaire et matriciel naif 
\item Élimination de Gauss-Jordan
\item Ajout d'un $\epsilon$-bruit.
\item Génération des mots de poids de Hamming fixé
\item Tri rapide sur les bits entre k et (k+i)
\item et de nombreux tests divers et variés comme l'étape de réduction de
BKW
\end{itemize}

\subsubsection{Phase finale du LPN en C}
J'ai implémenté en C, la phase finale qui consiste à faire une recherche
exhaustive en testant les mots de poids fixé, en $O(n)$. Via un calcul
d'approximation de coefficients de Fourier, expliquée en~\ref{SFT}.
\\
Cette phase est donc composée d'une recherche exhaustive sur des mots de
poids de Hamming petit fixé. Pour paralléliser cette implémentation, j'ai
mis en bijection les mots de poids de Hamming fixé et l'intervalle
d'entier $[1, C_{n}^{w}]$, de ce fait les itérations deviennent
entièrement indépendantes, permettant une parallélisation à une première
échelle via MPI.
À une deuxième échelle et de manière transparente, j'ai utilisé openMP
pour paralléliser sur plusieurs coeurs les boucles que doivent faire
chaque esclave. 
\\\\
Autre avantage : l'utilisation de MPI permet de broadcaster facilement
l'instance du LPN construite par le maitre à toutes les machines de la
grappe. En pratique le calcul a été fait sur 8 machines de
l'ENS\footnote{The super-new spice machines}(2x Intel Xeon E5649 @
2.53GHz).  
\begin{figure}
\caption{Temps d'exécution en dimension n avec un secret k-sparse}
\begin{center}
\begin{tabular}{|c |c | c | c | c |c|c| }
\hline
   kn & 32 & 64 & 128 & 256 & 512 & 768 \\
\hline
 2  & &4 & 5 & 6 & &\\
\hline
  3 & &7 & 8 & 9 & &\\
\hline
 4 & & & & & & \\
 \hline
5 & & & & & & \\
 \hline
6 & & & & & & \\
\hline 
\end{tabular}
\end{center}
\end{figure}


\subsection{Sage : c'est le bien}
 Une fois compilé (9 h tout de même\dots), Sage est particulièrement efficace
pour tester rapidement des idées : je pense à un facteur de compression
sur la taille du code
entre 10 et 100. De plus Sage
implémente plus ou moins l'état de l'art des algorithmes classiques : multiplication
matricielle, SAT-solver etc\dots  


\subsubsection{Systèmes polynomiaux}\label{ISAT}
Le but fut d'implémenter ce que nous avons proposé en~\ref{SAT}
\\\\
En Sage, on déclare l'anneau dans lequel on travaille, la méthode de
résolution du système et c'est fini. Autrement dit l'essentiel du travail
est de générer les équations : de traiter les données. Étant donné que la
complexité de SAT et Groëbner est très mal connue, le but était
uniquement d'effectuer un test, pour savoir si on se trouvait dans une
zone ou les méthodes usuelles se "comportent bien", ou pas\footnote{Je
présente ici chronologiquement ce que j'ai compris.}.
J'ai pris une instance du LPN de taille 32, générée en C, et ensuite j'ai
traité ces données via OCaml pour les formater pour Sage\footnote{Il
était possible de tout faire directement en Sage, mais je n'avais pas
fait d'OCaml depuis plusieurs semaines!}. Les équations polynomiales
étaient toutes de degrés 2 : produit des équations $2.i$ et $2.i+1$
linéaire du problème LPN initial.
\paragraph{Résolution via Groëbner}
Exécuté sur ma machine plusieurs heures, ça n'a pas terminé. Peut-être 
que le grand nombre de variables dans chaque équation est la cause de
l'échec de la résolution via
bases de Groëbner.
\paragraph{Résolution via SAT}
Via SAT, l'instance a été résolue en une dizaine de minutes. Ceci
correspond environ au temps necessaire à une recherche exhaustive, c'est
trop lent pour être envisageable, aucun miracle n'est arrivé. 
\\
Après avoir regardé
un peu l'état de l'art du SAT solving, notamment \cite{Mezard}, il semble qu'une fois encore la
structure des entrées liées au LPN est très différente des données
classiques que l'on donne au SAT solver : la plupart du temps il y a un
nombre très réduit de littéraux dans chaque clause, bien sur on peut se
ramener à ce cas en introduisant de nouvelles variables, mais alors le
rapport $\alpha=\frac {\sharp Clauses}{\sharp Variables}$ devient plus grand.
Or, \cite{Dubois} étudie que lorsque le nombre de variables tend vers
l'infini, presque toutes (au sens probabiliste) les
instances avec $\alpha > 4.506$ sont insatisfiables. Ce sont des
"hard-SAT"\footnote{Beaucoup de cycles dans le graphe de représentation}.

Ces lectures a posteriori\footnote{Toujours la difficulté de savoir s'il
est plus rapide d'implémenter pour tester, ou de trouver une biblio sur
le sujet} justifient que cette approche ne passera pas pour
des paramètres plus gros. 

\subsubsection{Réduction du LPN au Short Vector Problem}\label{Impsvp}
En \ref{SVP} on a vu une proposition de réduction du LPN au SVP dans un
sous-espace vectoriel du $\mathbb{Z}/2\mathbb{Z}$ espace vectoriel
$(\mathbb{Z}/2\mathbb{Z})^n$. L'implémentation choisie est celle avec une
réduction de réseau pour trouver le plus court vecteur. Cependant LLL
est fait pour des vecteurs dans réels, cependant on peut considérer les
vecteurs de $(\mathbb{Z}/2\mathbb{Z})^n$ comme des vecteurs réels,
effectuer LLL, qui ne fait que des combinaisons linéaires, puis se
replonger dans $GF(2)^n$.
\\ 
On remarque qu'il y a une relative corrélation entre un vecteur court de
$GF(2)^n$ avec la distance de Hamming, et avec la norme 2. De plus
expérimentalement, LLL donne des vecteurs à valeur très majoritairement
dans $\{-1,0,1\}$, donc cette méthode\footnote{Piquée à Martin Albrecht}
est expérimentalement justifiée.
\\
Le principal bottleneck de cette méthode est le facteur d'approximation
de LLL, qui trouve un plus court vecteur à un facteur au plus 
$(1.08)^{\frac {n-1} {2}}$. Ainsi très rapidement lorsque la
dimension augmente, ou que le bruit augmente trop, LLL n'assure plus
de trouver
un vecteur suffisament court, et bien que l'algorithme soit
très rapide, il n'est plus correct.
En effet il faut que : $$1.08^{\frac {n-1} {2}}HammingWeight(\nu)\ll1/2
$$ 
 En pratique il faut alors l'éxecuter
de nombreuses fois pour trouver le bruit.On 
peut le voir dans le tableau de performance ci-dessous. Les tests sont
réalisés sur ma machine personnelle (AMD V120 @ 2.2GHz)
\\
\begin{figure}
\caption{Complexité en temps pour essayer une instance}
\begin{center}
\begin{tabular}{|c |c | c | c | c | }
\hline
   & 32 & 64 & 96 & 128 \\
\hline
 0.01  & &4 & 5 & 6 \\
\hline
  0.05 & &7 & 8 & 9 \\
\hline
 0.1& & & &\\
 \hline
 \end{tabular}
\end{center}
\end{figure}

\begin{figure}
\caption{Nombre d'instances nécessaires avant réussite}
\begin{center}
\begin{tabular}{|c |c | c | c | c | }
\hline
   & 32 & 64 & 96 & 128 \\
\hline
 0.01  & &4 & 5 & 6 \\
\hline
  0.05 & &7 & 8 & 9 \\
\hline
 0.1 & & & & \\
 \hline
 \end{tabular}
\end{center}
\end{figure}

\bibliographystyle{alpha}
\bibliography{biblio}
\end{document}

